# Core codes and the prompt used for LLM-based data augmentation for the work "Leveraging Category-Aware MoE and Fine-Grained Sequence Feature Learning with LLMs for Enhanced Person-Job Fit"

## Abstract
Person-Job Fit (PJF) effectively connects job seekers with potential positions by evaluating their matching degree. Recent approaches have employed multiple neural networks to enrich text encoding and explored various candidate-job interaction strategies. Despite these advancements, they still face challenges, particularly in handling low-quality job descriptions and similar candidate-job pairs, which impairs model performance. To address these problems, this paper proposes a large language model (LLM) based method with two novel techniques: an LLM-based data augmentation technique to enhance the low-quality job descriptions, and a category-aware Mixture of Experts (MoE) module utilizing job and candidate category information to assist in identifying similar candidate-job pairs. The LLM-based data augmentation polishes and rewrites short job descriptions by utilizing chain-of-thought (COT) prompts. Meanwhile, the MoE module incorporates category embeddings to dynamically assign weights to the experts and takes the fine-grained historical interaction encoding as input to learn more distinguishable patterns for similar candidate-job pairs. We conduct offline experiments on real-world data and online A/B tests within Alibabaâ€™s recruitment platform. Our method outperforms the existing state-of-the-art method by 1.7% and 4.8% in AUC and GAUC respectively, and achieves a relative improvement of 19.4% in CTCVR for the online tests, saving tens of millions in headhunting expenses.